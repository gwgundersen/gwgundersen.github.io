---
title: Lecture 4 - Deep Learning II
layout: orf525
date: 2017-02-16
---

# Review

> **Definition.** We say $\sigma$ is discriminative if a measure, e.g. density function, $\mu \in M([0,1]^n)$ has:
>
> $$
> \int_{[0,1]^n} \sigma(y^{\top} x + \theta) du(x) = 0
> $$
>
> Where
>
> $$
> [0,1]^n = \{x_1, ..., x_n \in \mathbb{R}^n | 0 \leq x_i \leq 1, \forall i = 1, ..., n\}
> $$
>
> For all $y \in \mathbb{R}^n$, $\theta \in \mathbb{R}$ implies $\mu \equiv 0$

# Universal Approximation Theorem

**Kolmogorov, 1957**

Let $\sigma$ be discriminative, given $\forall f \in C([0,1]^n)$ and $\epsilon > 0$, i.e. a continuous function on $[0,1]^n$. Then there exists:

$$
G(x) = \sum_{j=1}^{N} \alpha_j \sigma(y_j^{\top} x + \theta_j)
$$

Such that:

$$
|G(x) - f(x)| \leq \theta
$$

For all $x \in [0,1]^n$.

$$
\mathscr{F} = \{ \sum_{j=1}^{N} \alpha_j \sigma(y_j^{\top} x + \theta_j) | N \geq 1, \alpha_j, \theta_j \in \mathbb{R}, y_i \in \mathbb{R}^n \}
$$

In words, for all continuous functions $f$ defined on the unit hypersphere, there is a function $G$ that approximates it with arbitrary error. A one-layer neural network approximates $f$ with any precision.

### Proof

##### Fact

$\mathscr{F}$ is a linear subspace in $C([0,1]^n)$.

(Called "obvious" in class.)

##### Closure

Define the _closure_ of $\mathscr{F}$ as:

$$
\bar{\mathscr{F}} = \{ f \in C([0,1]^n) | \forall \epsilon \, \exists G(x) \in f \text{ s.t. } |f(x) - G(x)| \leq \epsilon \}
$$

Example:

$$
\mathbb{Q} = \text{rational numbers}
$$

$$
\bar{\mathbb{Q}} = \mathbb{R}
$$

In words, any real number can be approximated by a rational number.

##### Hahn-Banach theorem

Let $X$ be a linear space with norm $\|\cdot\|$ and $G$ be a linear subspace of $X$. Then for all bounded linear functions $f$ defined on $G$, there exists a bounded linear function $F$ on $X$ such that:

1. $F(x) = f(x)$ for all $x \in G$
2. $\|F\| = \|f\|$

Think about this in $\mathbb{R}^2$. You have a function (line). Can you imagine extending it into $\mathbb{R}^3$?

##### Corollary

Let $G \subset X$, $x_0 \in X$ such that the distance $G > 0$. Then there exists a bounded linear function on $X$ such that:

1. $f(x) = 0$ for all $x \in G$
2. $f(x) = d$, $x_0$

**Proof.**

For all $x \in \text{span({$x_0, G$}) $= A$}$, we can write uniquely:

$$
x = x^{\prime} + t x_0
$$

Where $x^{\prime} \in G$.

Define:

$$
g(x) = t \times \text{distance($x_0, G$)}
$$
    
Then $g(x)$ is linear in $A$.

$$
g(x_0) = \text{distance($x_1, G$)}
$$

And $g(x) = 0$ for all $x_0 \in G$

Moreover:

$$
|x| = |x^{\prime} + t x_0| = |t| \cdot |\frac{x^{\prime}}{t} + x_0|
$$

$$
\geq |t| \text{distance($x_0, t$)} = |g(x)|
$$

So $g$ is bounded and linear. Therefore, we can apply the Hahn-Banach theorem.

$$
\exists f \text{ that is linear on $X$ such that } f(x) = g(x) \text{ for all $x \in A$}
$$

#### By the corollary:

There exists a linear function $\mathscr{L}$ on $X$ such that:

$$
\mathscr{L} = 0
$$

For all $h \in \bar{\mathscr{F}}$ but $\mathscr{L} \not\equiv 0$

Assume $\bar{\mathscr{F}} \neq C([0,1]^n)$.




---
title: Deep Learning II - Universal Approximation Theorem, CNNs
number: 4
layout: orf525
date: 2017-02-16
---

# Review

> **Definition.** We say $\sigma$ is discriminative if a measure, e.g. density function, $\mu \in M([0,1]^n)$ has:
>
> $$
> \int_{[0,1]^n} \sigma(y^{\top} x + \theta) \mu(x) = 0
> $$
>
> Where
>
> $$
> [0,1]^n = \{x_1, ..., x_n \in \mathbb{R}^n | 0 \leq x_i \leq 1, \forall i = 1, ..., n\}
> $$
>
> For all $y \in \mathbb{R}^n$, $\theta \in \mathbb{R}$ implies $\mu \equiv 0$

# Universal Approximation Theorem

**Kolmogorov, 1957**

Let $\sigma$ be discriminative, given $\forall f \in C([0,1]^n)$ and $\epsilon > 0$, i.e. a continuous function on $[0,1]^n$. Then there exists:

$$
G(x) = \sum_{j=1}^{N} \alpha_j \sigma(y_j^{\top} x + \theta_j)
$$

Such that:

$$
|G(x) - f(x)| \leq \theta
$$

For all $x \in [0,1]^n$.

$$
\mathscr{F} = \{ \sum_{j=1}^{N} \alpha_j \sigma(y_j^{\top} x + \theta_j) | N \geq 1, \alpha_j, \theta_j \in \mathbb{R}, y_i \in \mathbb{R}^n \}
$$

In words, for all continuous functions $f$ defined on the unit hypersphere, there is a function $G$ that approximates it with arbitrary error. A one-layer neural network approximates $f$ with any precision.

### Proof

##### Fact

$\mathscr{F}$ is a linear subspace in $C([0,1]^n)$.

(Called "obvious" in class.)

##### Closure

Define the _closure_ of $\mathscr{F}$ as:

$$
\bar{\mathscr{F}} = \{ f \in C([0,1]^n) | \forall \epsilon \, \exists G(x) \in f \text{ s.t. } |f(x) - G(x)| \leq \epsilon \}
$$

Example:

$$
\mathbb{Q} = \text{rational numbers}
$$

$$
\bar{\mathbb{Q}} = \mathbb{R}
$$

In words, any real number can be approximated by a rational number.

##### Hahn-Banach theorem

Let $X$ be a linear space with norm $\|\cdot\|$ and $G$ be a linear subspace of $X$. Then for all bounded linear functions $f$ defined on $G$, there exists a bounded linear function $F$ on $X$ such that:

1. $F(x) = f(x)$ for all $x \in G$
2. $\|F\| = \|f\|$

Think about this in $\mathbb{R}^2$. You have a function (line). Can you imagine extending it into $\mathbb{R}^3$?

##### Corollary

Let $G \subset X$, $x_0 \in X$ such that the distance $G > 0$. Then there exists a bounded linear function on $X$ such that:

1. $f(x) = 0$ for all $x \in G$
2. $f(x) = d$, $x_0$

**Proof.**

For all $x \in \text{span({$x_0, G$}) $= A$}$, we can write uniquely:

$$
x = x^{\prime} + t x_0
$$

Where $x^{\prime} \in G$.

Define:

$$
g(x) = t \times \text{distance($x_0, G$)}
$$
    
Then $g(x)$ is linear in $A$.

$$
g(x_0) = \text{distance($x_1, G$)}
$$

And $g(x) = 0$ for all $x_0 \in G$

Moreover:

$$
|x| = |x^{\prime} + t x_0| = |t| \cdot |\frac{x^{\prime}}{t} + x_0|
$$

$$
\geq |t| \text{distance($x_0, t$)} = |g(x)|
$$

So $g$ is bounded and linear. Therefore, we can apply the Hahn-Banach theorem.

$$
\exists f \text{ that is linear on $X$ such that } f(x) = g(x) \text{ for all $x \in A$}
$$

#### By the corollary:

There exists a linear function $\mathscr{L}$ on $X$ such that:

$$
\mathscr{L} = 0
$$

For all $h \in \bar{\mathscr{F}}$ but $\mathscr{L} \not\equiv 0$

Assume $\bar{\mathscr{F}} \neq C([0,1]^n)$.

#### By representation theorem

$$
\mathscr{L} = \int_{[0,1]^n} h(x)\mu(x)
$$

For some $\mu \in M([0,1]^n)$

Analog: In $\mathbb{R}^d$, a linear function $f$ has:

$$
f(x) = \langle v, x \rangle = \sum_{j=1}^{d} v_j x_j
$$

For all $\mathbb{R}^d$.

Apply to $h(x) = \sigma(y^{\top} x + \theta)$

$$
\mathscr{L}(h) = \int_{[0,1]^n} \sigma(y^{\top} x + \theta) \mu(x) = 0
$$

For all $y, \theta$.

Since $\sigma$ is discriminatory, we know $\mu \equiv 0 \implies \mathscr{L} \equiv 0$ on $X$. Contradiction!

### Big picture of proof

> **Definition.** We say the activation function $\sigma$ is _sigmoidal_ if:
>
> $$
> \sigma(t) = \begin{cases} 1 & \text{as $t \rightarrow \infty$} \\ 0 & \text{as $t \rightarrow -\infty$}\end{cases}
> $$

#### Lemma

> Any bounded sigmoidal function is discriminatory.

**Proof.**

Let $\sigma_{\lambda}(x) = \sigma(\lambda(y^{\top} x + \theta) + \psi)$

$$
\sigma_{\lambda}(x) \rightarrow \gamma(x) = \begin{cases}
    1 & y^{\top} x + \theta > 0 & \text{As $\lambda \rightarrow \infty$} \\
    0 & y^{\top} x + \theta < 0 & \text{As $\lambda \rightarrow \infty$} \\
    \sigma(\psi) & y^{\top} x + \theta = 0

\end{cases}
$$

$$
0 = \int_{[0,1]^n} \sigma_{\lambda}(x) \mu(x) = \lim_{\lambda \rightarrow 0} \int_{[0,1]^n} \sigma_{\lambda}(x) \mu(x)
$$

By the bounded convergence theorem.

$$
= \int_{[0,1]^n} \gamma(x) \mu(x) = \sigma(\psi) \mu(\pi_{y, \theta}) + \mu(H_{y, \theta})
$$

Where:

$$
\pi_{y, \theta} = \{x | y^{\top} x + \theta = 0\}
$$

$$
H_{y, \theta} = \{x | y^{\top} x + \theta > 0\}
$$

$$
\implies \mu(H_{y, \theta}) = 0
$$

For all $y, \theta$.

<img src="{{ site.url }}/images/orf525/uat_proof.png" style="width: 350px;"/>

**Claim.** The above implies $\mu \equiv 0$

We define $F(h) = \int_{[0,1]^n} h(x) d\mu(x) \equiv 0$

$$
h(t) = \mathbb{1}_{[\theta, \infty)}
$$

$$
F(h) = \int h(y^{\top} x) d\mu(x) = 0 \impliedby \mu(H_{y, \theta}) + \mu(\pi_{y, \theta}) = 0
$$

$$
F(\cdot) \text{ is always zero for } h(t) = \mathbb{1}_{[\theta, \infty)}
$$

$$
\implies F(\cdot) \text{ is zero for any linear combination of $\mathbb{1}$ functions}
$$

$$
\sum_{i=1}^{n} \alpha_i \prod_{[\theta_i, \infty)}
$$

$$
\implies F(\cdot) \text{ is always 0}
$$

#### Question

So why bother using deep neural networks?

In math, there is no good explanation for _why_ deep nets work.

We know $N$ thought: how many neurons do we need. Maybe it's exponential as $\epsilon$ decreases?


# Convolutional neural networks

> **Definition.** 1-D convolution: Given a 1-D filter:
>
> $$
> g = \langle g_1, ..., g_k \rangle^{\top}
> $$
>
> The discrete convolution between a filter and a discrete function $f$ is:
>
> $$
> f = \langle f_1, ..., f_n \rangle
> $$
>
> $$
> (f * g)[n] = \sum_{i=1}^{m} f(m)g(n-m)
> $$

> **Definition.** 2-D convolution: Given a 2-D filter:
>
> $$
> g = \begin{bmatrix}
>     g_{11} & ... & g_{1n} \\
>     ... & ... & ... \\
>     g_{m1} & ... & g_{mn}
> \end{bmatrix}
> $$
>
> $$
> f = \begin{bmatrix}
>     f_{11} & ... & f_{1m} \\
>     ... & ... & ... \\
>     f_{m1} & ... & f_{mn}
> \end{bmatrix}
> $$
>
> $$
> (f * g)[j, k] = \sum_s \sum_t f(s, t) g(j-s, k-t)
> $$

> **Definition.** A NN layer is a _convolutional layer_ if it has weights $W^k$ that can be represented by convolutions:
>
> $$
> \sigma(y^{\top} x + \theta) \rightarrow \sigma(g * x + \theta)
> $$

> **Definition.** A _convolutional neural network_ is an NN with convolutional layers.

---
title: Lecture 5 - Deep Learning III
layout: orf525
date: 2017-02-21
---

# Review

- Convolutional unit: a nueral computational unit which has the form:

    $$
    \sigma(g * x)[i] + \beta_0)
    $$

    For some filter $g$, index $i$, and constant $\beta_0$.

- Convolution:

    $$
    (g * x)(n) = \sum_m x(m)g(n-m)
    $$

# Convolutional Neural Networks (CNNs) continued...
    
## One-layer CNN

<img src="{{ site.url }}/images/orf525/one-layer-1d-cnn.jpg" style="width: 500px;"/>

Where filter $g = \langle g_1, g_2, g_3 \rangle^{\top}$

And $x = \langle x_1, x_2, ..., x_6 \rangle^{\top}$

$$
y_1 = \sigma((g * x)(3))
$$

$$
y_2 = \sigma((g * x)(4)) = \sigma(g_1 x_4 + g_2 x_3 + g_3 x_2)
$$

<div style="color: red;">I think the above is wrong, but I am leaving it since it is what Junwei wrote in class. I think the real output is:

$$
    y_2 = \sigma((g * x)(4)) = \sigma(g_1 x_3 + g_2 x_2 + g_3 x_1)
$$

I also think his diagram is slightly wrong. His weights should be reversed, since the convolution operator "flips" the filter.
</div>

# 2-dimensional single-layer CNN

<img src="{{ site.url }}/images/orf525/cnn-2d-filter.jpg" style="width: 150px;"/>

Filter $g \in \mathbb{R}^2$:

$$
g = \begin{bmatrix}
    g_1 & g_2 \\
    g_3 & g_4
\end{bmatrix}
$$

$$
(g * x)(j, k) = \sum_{s, t} x(s, t)g(j-s, k-t)
$$

$$
(g * x)(3, 3) = g_1 x_{22} + g_2 x_{21} + g_3 x_{12} + g_4 x_{22}
$$

Choose index $j, k$ so that the filter is inside the input.

### Remark

- Fully connected NN has **dense** parameters
- Hard to train
- Easy to overfit
- CNN's parameters are fixed within the filter
- Easier to train
- Avoids overfitting
- GWG: A kind of regularization

## 2-dimensional CNN with multiple filters

- A single convolutional layer can have multiple filters.
- Intuition: each filter provides a different perspective of the image.
- "Stack" different filters into tensor

<img src="{{ site.url }}/images/orf525/cnn-layers-hstacked.jpg" style="width: 350px;"/>

- Han Liu asked in class: Why do we not stack them vertically?
- Junwei's answer: That is also O.K., but a tensor or 3D matrix is easier for a GPU to compute.

<img src="{{ site.url }}/images/orf525/cnn-layers-vstacked.png" style="height: 350px;"/>

### Ex: Input as a 3D matrix

- What happens if our input is a 3D matrix?
- We apply the filter to each 

<img src="{{ site.url }}/images/orf525/cnn-3d-filter.jpg" style="width: 250px;"/>

- The output is the sum over all planes

$$
(g * f)(s, t) = \sum_{j=1}^{n} (g * f_j)(s, t)
$$

<span style="color: red;">Is this true? My understanding is that it is optional. You can have a convolutional layer whose input is 6-deep and output is also 6-deep. This suggests to me that the output is always a single layer.</span>

## Max pooling

- We consider a max pooling "filter" (it is not really a CNN filter) $g_{max}$ to be a maximum function. The max pooling between $g_{max}$ and $f$ is:

    $$
        (f * g_{max})(n) = \max_{n-m \in \{1,...,k\}} f(m)
    $$

- Where $k$ is the dimension of $g_{max}$.
- Why? Dimension reduction.

## Ex: LeNet 5

- Yann LeCun, 1998
- Applied to the MNIST dataset: images of handwritten digits
- Just do a [Google image search](https://www.google.com/search?q=lenet&tbm=isch) for the diagram.

## Ex: Residual network

- Microsoft Research, 2015
- Applied to ImageNet dataset: 1.2M images
- RN has 3.5% error. <span style="color: red;">I believe this is the MSRA network in the first deep learning lecture.</span>

<img src="{{ site.url }}/images/orf525/residual-nn-layer.jpg" style="width: 500px;"/>

- Where $H(x) = F(x) + x$

- MSR network:

<img src="{{ site.url }}/images/orf525/residual-nn-msrn.jpg" style="width: 700px;"/>

# Training NNs

- Ex: a neural network that takes a 32 by 32 image and labels it as a cat or dog.
- So it takes as input $(x_1, y_1), ..., (x_n, y_n)$ where $x_i \in \mathbb{R}^{32 \times 32}$ and $$y_i \in \{ \text{cat}, \text{dog} \}$$.
- Use MLE:

$$
\max_f g(f, \{ x_i, y_i \}_{i=1}^n)
$$

$$
= \max_f \sum_{i=1}^n \log(1 + e^{y_i f(x_i)})
$$

- Where $f$ is a neural network:

$$
f = \sigma(W^{(l)} \sigma(W^{(l-1)} ... \sigma(W^{(1)} x)...)
$$

$$
\max_{\omega} = \frac{1}{n} \sum \log (1 + e^{y_i \sigma(...)})
$$

- Where $W \in (W^{(l)} ... W^{(1)})$

- <div style="color: red;"><a href="https://en.wikipedia.org/wiki/Maximum_likelihood_estimation">Review of MLE from Wikipedia</a>: "In statistics, maximum likelihood estimation (MLE) is a method of estimating the parameters of a statistical model given observations, by finding the parameter values that maximize the likelihood of making the observations given the parameters." There is nothing deep by this notation. It completely matches your intuition that we just change the parameters until we find the ones that make the outputs the most likely.</div>

# Stochastic gradient descent

### Gradient descent

<img src="{{ site.url }}/images/orf525/gradient-descent.jpg" style="width: 250px;"/>

In general:

$$
w_t = w_{t-1} + \mu \nabla g(w_{t-1})
$$

### Problem for deep learning training:

Loss function for $n$ samples:

$$
g(x) = \sum_{i=1}^{n} g(w, x_i y_i)
$$

In words: this summation can have many, many terms (millions or even billions) for every single iteration of gradient descent.

### SGD

- Randomly sample $i \sim \text{Uniform}(\{1, ..., n\})$
- Perform gradient update:
    
    $$
    w_t = w_{t-1} + \mu \nabla g(w_{t-1})
    $$

- Why does this work?

    $$
    \mathbb{E}_i(w_t) = \mathbb{E}_i(w_{t-1}) + \mu \frac{1}{n} \sum_{i=1}^{n} \nabla g(w_i, x_i, y_i)
    $$

- In words: it converges to the same thing.


### Backpropagation

- Maximize $g(x) = l(f(w))$ where $f(w)$ is our network.
- So $\nabla g(w) = \frac{\partial l}{\partial f} \frac{\partial f}{\partial w}$ by the chain rule
- The problem is computing $\frac{\partial f}{\partial w}$
- (The derivative of the loss function is not hard)

##### Example

$$
    \frac{\partial f}{\partial w_k^{(l)}} = \frac{\partial f}{\partial z^{(l)}} \frac{\partial z^{(l)}}{\partial w_k^{(l)}}
$$


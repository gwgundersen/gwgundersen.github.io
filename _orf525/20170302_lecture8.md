---
title: Lasso, nonparametric regression
number: 8
layout: orf525
date: 2017-03-02
---

# Review

Last lecture

- High-dimensional regression estimators
- Ridge (L2) $\rightarrow$ Bridge (Lp) $\rightarrow$ Lasso (L1, sparsity) $\rightarrow$ SQRT-lasso (robust optimization interpretation)
- Elastic Net: 2 tuning parameters that select either lasso or ridge
- Regularization path plots
- Persistency theory of lasso

# Persistency theory of lasso

Population risk:

$$
R(\vec{\beta}) = \mathbb{E}_{Y,X} = (\hat{\vec{Y}} - \vec{\beta}^{\top}\mat{X})^2
$$

Empirical risk (the risk you observe):

$$
\hat{R}(\vec{\beta}) = \frac{1}{n} \sum_{i=1}^{n} (\hat{\vec{Y}_i} - \vec{\beta}^{\top}\mat{X}_i)^2
$$

Lasso:

$$
\hat{\vec{\beta}}^{L} = \arg\min_{\lVert \vec{\beta} \rVert_1 \leq L} \hat{R}(\vec{\beta})
$$

In words, we are saying that lasso finds the coefficients $\beta$ that minimizes the empirical risk.

Oracle:

$$
\vec{\beta}^* = \arg\min_{\lVert \vec{\beta} \rVert_1 \leq L} R(\vec{\beta})
$$

The oracle is not empirically computable. It is the target in which we are interested.

> **Definition.** $\hat{\vec{\beta}}$ is **persistent** with respect to $Bn$ if $R(\hat{\vec{\beta}}) - R(\vec{\beta}^*) \stackrel{\text{P}}{\rightarrow} 0$ as $n \rightarrow \infty$.

<span style='color: red;'>In words, I believe this means that our estimated coefficients are "good enough". As the number of samples we see increases, probability of the difference between the risk of our estimated coefficients and the oracle $\vec{\beta}^*$ goes to 0, as estimated by the population risk $R$.</span>

<span style='color: red;'>QUESTION: What is $Bn$?</span>

> **Theorem.** Lasso is persistent. Let $\|\vec{Y}_i\| \leq B$, $\max_i \lVert \vec{X}_i \rVert \leq B$. Then:
>
> $$
> \mathbb{P}(R(\hat{\vec{\beta}}) - \inf_{\lVert \beta \rVert_1 \leq L} R(\vec{\beta}) \leq 2(1 + L)^2 \sqrt{\frac{2B^4 \log{\frac{2 d^2}{\delta}}}{n}} \geq 1 - \delta)
> $$
>
> For all $\delta \in (0,1)$ and with probability $1 - \delta$ upper bound.

### Proof

(Han Liu says this proof uses "many standard tricks".)

Goal: we want to control $R(\hat{\vec{\beta}}) - R(\vec{\beta}^*)$ but $R(\hat{\vec{\beta}})$ is a stochastic quantity. <span style='color: red;'>(I believe it is stochastic because it depends on the samples we see.)</span> It is hard to deal with. Here is one standard trick in these kinds of proofs:

$$
R(\hat{\vec{\beta}}) - R(\vec{\beta}^*) = R(\hat{\vec{\beta}}) - \hat{R}(\hat{\vec{\beta}}) + \hat{R}(\hat{\vec{\beta}}) - R(\vec{\beta}^*)
$$

Observe that $\hat{R}(\hat{\vec{\beta}}) \leq \hat{R}(\vec{\beta}^*)$. <span style='color: red;'>(QUESTION: Wait. Why is this true? I would have expected the opposite. That the risk (average error) would be higher for our estimated parameters $\hat{\vec{\beta}}$ than for our oracle parameters $\vec{\beta}^*$. If I had to provide a guess as to why not, it is because our estimated parameters only look at realizations.)</span>

$$
\leq R(\hat{\vec{\beta}}) - R(\vec{\beta}^*) = R(\hat{\vec{\beta}}) - \hat{R}(\hat{\vec{\beta}}) + \hat{R}(\vec{\beta}^*) - R(\vec{\beta}^*)
$$

$$
\leq 2 \sup_{\lVert \vec{\beta} \rVert_1 \leq L} |R(\vec{\beta}) - \hat{R}(\vec{\beta})|
$$

<span style='color: red;'>QUESTION: I do not understand the above step.</span>

Where $R(\vec{\beta}) - \hat{R}(\vec{\beta})$ is a deterministic quantity. We only need to control the $2 \sup$ term; call it $*$.

**Simplifying notation:**

Let $\vec{z} = (\vec{Y}, \vec{X}^{\top})^{\top}$ and $\vec{z}_i = (\vec{Y}_i, \vec{X}_i^{\top})$

Let $\vec{\gamma} = (-1, \vec{\beta}^{\top})^{\top}$

So we have:

$$
R(\vec{\beta}) = \mathbb{E}(\vec{Y} - \beta^{\top} \mat{X})^2
$$
---
title: Deep Learning I - Introduction
number: 3
layout: orf525
date: 2017-02-14
---

# Introduction

### Applications

- Computer vision
- Self-driving cars
- AlphaGo
- Super Mario
- Google Translate


### History

(Not invented overnight)

- Invention of the computer, 1940s
- Artificial neural network (NN), 1957
    - Frank Rosenblatt
    - Mark 1 Perceptron
    - Not very effective at the time
- Support vector machine (SVM), 1993
    - Vladimir Vapnik
    - At the time, SVMs beat NNs
- Deep learning, 2006
    - George Hinton and others
    - Idea: add more layers to NNs
    - Why didn't we do this before? Computers weren't powerful enough

### Three aspects of deep learning

1. Architecture
    - Supervised
        - Fully connected NN
        - Convolutional NN
        - Recurrent NN
        - ...and many variations
    - Unsupervised
        - Autoencoder
        - Deep belief network
        - Generative adversarial network
        - ...and many variations
2. Training
    - Stochastic gradient descent
    - Dropout
    - Pretraining
3. Applications (examples)


### Convolutional neural networks (CNNs): ImageNet dataset

ImageNet dataset
- 1K categories
- 1.2M training images
- 150K testing images
- NN results on classifying ImageNet data made NNs famous

| Architecture | NEC-UIUC | Supervision | VGG  | MSRA |
|--------------|----------|-------------|------|------|
| Year         | 2011     | 2012        | 2014 | 2015 |
| Error        | 25%      | 20%         | 16%  | 3.5% |

- 3.5% is better than humans

Intuition for CNNs

- Hierarchical
- Low-level features are first extracted, then higher level features

Applications

- Google Goggles
- Neural Style
- Deep dream
- Automatic colorization
- Snapchat filters
- Meitu filters
- PRISMA


### Recurrent Neural Networks (RNNs): Natural language processing

- `word2vec` - Distributional hypothesis of meaning
- Big idea: represent words as vectors in some high-dimensional meaning space; words that are geometrically closer are semantically similar

Applications

- Google Translate
- Automatic image captioning
- YouTube captioning
- Fake algebra paper generation


### Deep reinforcement learning

`Agent state` > `Agent action` > `Environment change` > `Reward` > `New observations`

Applications

- Atari pong
- AlphaGo
- Libratus (CMU poker)

# Fully connected NN

### Basic unit of NN

> **Definition** of _neural computational unit_:
>
> $$
> f(\vec{x}) = \sigma(\vec{\beta}^{\top} \vec{x} + \beta_0)
> $$
>
> Where $\vec{\beta} = (\beta_1, ..., \beta_k)$
>
> And $\sigma(\cdot)$ is the _activation function_

<img src="{{ site.url }}/images/orf525/neural_unit.png" style="width: 400px;"/>

### Typical activation functions

- _Linear_: $\sigma(t) = t$

    <img src="{{ site.url }}/images/orf525/linear.png" alt="Linear activation function." style="width: 200px;"/>

- _Threshold_: 
    $$
    \sigma(t) = \begin{cases} +1 & t \geq 0 \\ -1 & t < 0 \end{cases}
    $$

    <img src="{{ site.url }}/images/orf525/threshold.png" alt="Threshold activation function." style="width: 200px;"/>

- _Sigmoid_: $\frac{1}{1 + e^{-t}}$

    <img src="{{ site.url }}/images/orf525/sigmoid.png" alt="Sigmoid activation function." style="width: 200px;"/>

- _Rectified linear units (ReLU)_: 
    $$
    \sigma(t) = \begin{cases} t \\ 0 \end{cases}
    $$

    <img src="{{ site.url }}/images/orf525/relu.png" alt="ReLU activation function." style="width: 200px;"/>

### Example

Let $\sigma$ be the threshold activation function. Then the neural computational unit is a linear classifier:

<img src="{{ site.url }}/images/orf525/threshold_classifier.jpg" style="width: 300px;"/>

### Recursive composition of units

<span style="color: red;">CONFIRM: Notation in this section.</span>

> **(Informal) Definition** of _neural network_:
>
> $F = \{f(x) : \text{$f$ can be represented by composition of units}\}$

<img src="{{ site.url }}/images/orf525/combination_neural_units.png" style="width: 300px;"/>

Soo $\sigma$ for our unit (generalizing a bit) is:

$$
\sigma(\beta^{(1)}_{10} + \beta^{(1)}_{11} x_1 + ... + \beta^{(1)}_{1K} x_k)
$$

And the weights can be represented as a matrix:

$$
W^{(1)} = \begin{bmatrix}
    B_{00} & ... & B_{1k} \\
    ... & ... & ... \\
    B_{N0} & ... & B_{NK}
\end{bmatrix}
$$

Therefore:

$$
\vec{x} = (x_1, ..., x_k)^{\top}
$$

$$
W^{(1)} \vec{x} = \begin{bmatrix}
    \vec{\beta}_0^{\top} \vec{x} \\
    ... \\
    \vec{\beta}_N^{\top} \vec{x}
\end{bmatrix}
$$

$$
\sigma(W^{(1)} \vec{x}) = (\sigma(W^{(1)}_1 \vec{x}), ... \sigma(W^{(1)}_N \vec{x}))^{\top}
$$

Output of NN:

$$
f(\vec{x}) = \sigma(W^{(2)} \sigma(W^{(1)} \vec{x}))
$$

> **(Formal) Definition** of _neural network_:
>
> $$
> F = \{f(x): f(x) = \sigma(W^{(k)} \sigma(... \sigma(W^{(1)} \vec{x}))) \}
> $$
>

> **Definition** of _fully connected neural network_:
>
> If the weight matrices of an NN have no zero entries, it is fully connected.

> **Definition** of the _number of layers_:
>
> The number of recursive layers.

#### Remark

We often talk about all the weights as a "tensor" $W$, even though each matrix of weights may be a different size.

<img src="{{ site.url }}/images/orf525/weights_tensor.png" style="width: 300px;"/>


# 2.1 Deep neural nets

> **Definition** of a _deep neural network_:
>
> Any network with 3 or more layers.
> (We will show why this is true later.)

Why do we want deep networks? To make the classifier complex.

### Example

Given images $X_1, ..., X_n$ and labels $Y_1, ..., Y_n$, we want to maximize the log likelihood:

$$
\max_{f \in F} \log{\prod_{i=1}^{n} \mathbb{P}(Y=y_i | X = x_i)}
$$

$$
= \min_{f \in F} \sum_{i=1}^{n} \log{(1 + e^{-y_i f(x_i)})}
$$

In words: minimize function by finding the best $f$ in the family $F$.

But there are problems.

- First, $F$ is very large.
- The objective function is very nonconvex. The computation is hard.

Diagrammatically, in an extremely high dimensional space, e.g. $\mathbb{R}^{900}$, the manifolds for cat and dog might be very similar (relative to the manifolds for other images); we need a function that can discriminate between the two:

<img src="{{ site.url }}/images/orf525/manifold.png" style="width: 300px;"/>


### Theorem: Universal Approximation Theorem

<span style="color: red;">CONFIRM: Notation in this section.</span>

Kolmogorov, 1957

**Informal argument**: A 2-layer NN can approximate any continuous function if the activation function is "good enough".

> **Definition** of a _discriminative activation function $\sigma$_: $\sigma$ is discriminative if a measure on $\mu \in M([0,1]^n)$ has:
>
> $$
> \int_{[0,1]^n} \sigma(\vec{y}^{\top} \vec{x} + \theta) d\mu(x) = 0
> $$
>
> For $\forall y, \theta \implies \mu = 0$

### Theorem

Let $\sigma$ be discriminative. Then $\forall f \in C([0,1]^n)$ where $C$ indicates all continuous functions, then $\forall \epsilon > 0, \forall$ two-layer NNs, $G(x) = \sum_{j-1}^N \alpha_j \sigma(y_i^{\top} x + \theta_j)$ s.t. 

$$|G(x) = f(x)| < \epsilon$$

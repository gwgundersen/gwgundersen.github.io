---
title: Bridge and Lasso estimator
number: 7
layout: orf525
date: 2017-02-23
---

# Admin

- HW1 is assigned
    - It is rather theoretical
    - Start early and work backwards (start with the last problem)

# Review

- Statistical learning
	- Ordinary least squares
	- Basis expansion
	- Ridge estimator
	- Data splitting
	- Cross validation
- Deep learning
    - Fully connected NN
    - Convolutional NN
    - Recurrent NN
    - Stochastic gradient descent

# Bridge estimator

### (Review): ridge estimator

From [Metacademy](https://metacademy.org/graphs/concepts/ridge_regression):

> A problem with vanilla linear regression is that it can overfit, by forcing the learned parameters to match all the idiosyncrasies of the training data. Ridge regression, or regularized linear regression, is a way of extending the cost function with a regularizer which penalizes large weights. This leads to simpler solutions and often improves generalization performance. This idea of regularization can be used to improve the generalization performance of many other statistical models as well. 

$$
\hat{\vec{\beta}}^{\text{Ridge}} = \arg\min_{\vec{\beta}} = \lVert \vec{Y} - \vec{X} \vec{\beta} \rVert_2^2 + \lambda \lVert \vec{\beta} \rVert_2^2
$$

### Bridge estimator

$$
\vec{\beta}^{\text{Bridge}} = \arg\min_{\vec{\beta}} \lVert \vec{Y} = \vec{X} \vec{\beta} \rVert_2^2 + \lambda \lVert \vec{\beta} \rVert_p^p
$$

Here, we define:

$$
\lVert \vec{\beta} \rVert_p = (|\beta_1|^p, ..., |\beta_2|^p)^{\frac{1}{p}}
$$

If $p = 2$, we have the L2 norm.

#### Remark

- If $1 \leq p < \infty \implies \lVert \cdot \rVert_p$ is a norm.
- If $0 < p < 1 \implies \lVert \cdot \rVert_p$ is _not_ a norm.

#### Geometrically

<span style='color: red;'>Called <a href='https://en.wikipedia.org/wiki/Lp_space'>$L^P$ space</a>. Below, each geometric shape is a unit shape, i.e. the value of the norm is 1. For example, for $p = 2$, the radius is 1. For $p = 1$, the sum of the two components must be 1, and so on.</span>

<img src="{{ site.url }}/images/orf525/bridge-estimator.png" style="width: 700px;"/>

# Lasso estimator

- 1996
- A special case of the bridge estimator
- **L**east **A**bsolute **S**hrinkage and **S**election **O**perator

> **Definition.** The bridge estimator with $p = 1$:
>
> $$
> \hat{\vec{\beta}}^{\lambda}
> = \arg\min_{\vec{\beta}} \lVert \vec{Y} - \vec{X} \vec{\beta} \rVert_2^2 + \lVert \vec{\beta} \rVert_1
> $$

The above equation is in **regularization form**.

Why is the lasso nice?

- _Selection_ is what makes lasso especially different.
- Geometrically, lasso should converge to a fixed point since it is "pointy"
- _Shrinkage_: move the estimator close to 0

For all $\lambda$, there exists $t$ such that:

$$
\hat{\vec{\beta}}^{\lambda} = \arg \min_{\lVert \vec{\beta} \rVert_1 \leq t} \lVert \vec{Y} - \vec{X} \vec{\beta} \rVert_2^2
$$

The above equation is in **constraint form**.

There is a one-to-one mapping between $t$ and $\lambda$. $\lambda$ from small to large corresponds to $t$ from large to small.

<img src="{{ site.url }}/images/orf525/lasso-estimator.png" style="width: 450px;"/>

# Sparsity

- Many elements of $\vec{\beta}$ are 0.
- This is a kind of variable selection.
- For example

	$$
	f(\vec{x}) = \beta_1 x_1 + \beta_2 x_2 + ... + \beta_d x_d
	$$

	If $\beta_j = 0$, then the $j$th feature or variable $x_j$ is **not selected** since its coefficient is 0.

### In the bridge family:

- If $0 \leq p \leq 1$ then sparsity
	- <span style="color: red;">My intuition for this is that the constraint forces some $\beta_j$'s to be 0 because otherwise it would not "fit" in the hypercube.</span>
- If $1 \leq p \leq \infty$ then convexity
	- <span style="color: red;">See the drawing for when $p = 0.5$. It should be visually obvious that that plot is not convex.</span>
- With lasso, $p = 1$, so it is both sparse and convex!

# Ridge vs. Lasso

### Remark on collinearity

> **Definition.** _Collinearity_ is a phenomenon in which two or more predictors are highly correlated.

Remember our ridge estimator:

$$
\hat{\vec{\beta}}^{\lambda} = (\vec{X}^{\top} \vec{X} + \lambda \vec{I_d})^{-1} \vec{X}^{\top} \vec{Y}
$$

The term $\lambda \vec{I_d}$ handles collinearity well because no value on the diagonal can be less than $\lambda$.

(Also can say: the data is rank deficient.)

### Comparison

- Ridge
	- Not sparse
	- Handles collinearity
- Lasso
	- Sparse
	- Does _not_ handle collinearity

<img src="{{ site.url }}/images/orf525/lasso-collinearity.png" style="width: 400px;"/>

# Elastic net

**Idea:** combine advantages of lasso and ridge.

$$
\hat{\vec{\beta}}^{\text{Elastic}} = \arg \min_{\vec{\beta}}
\lVert \vec{Y} - \mat{X} \vec{\beta} \rVert
+
\lambda [\alpha \lVert \vec{\beta} \rVert_1 + (1 - \alpha) \lVert \vec{\beta} \rVert_2^2]
$$

In other words

- $\alpha = 1 \implies $ lasso
- $\alpha = 0 \implies $ ridge

Rule of thumb: $\alpha = 0.6$. Why? This works well in practice.

# Understanding parameters

###  Regularization path plot

As $\lambda$ changes, show how the solution changes (lasso, ridge, elastic net)

<img src="{{ site.url }}/images/orf525/regularization-path.png" style="width: 400px;"/>

Each spline is an estimated coefficient $\hat{\beta}_i$ as $\lambda$ changes. A single slice represents all the values for the vector $\vec{\hat{\beta}}$. If a spline does not exist at a slice, the value for the missing coefficient is just 0.

### Two-stage model diagnosis approach

- Step 1
	- Use $\alpha = 1$
	- Fit a full lasso path
	- Visualize the regularization path
- Step 2
	- Use $\alpha = 0.6$
	- Fit the regularization path
	- Examine whether there is significant change in the fitted path
		- _Yes change?_ Use $\alpha = 0.6$ (Elastic)
		- _No change?_ The system does not have much multi-collinearity, use $\alpha = 1$ (lasso)

For finding the best $\lambda$, use cross-validation.

# SQRT-lasso

SQRT-lasso is an equivalent form of lasso

$$
	\hat{\vec{\beta}} 
	= \arg\min_{\vec{\beta}} 
	\lVert \vec{Y} - \mat{X} \vec{\beta} \rVert_2 + \lambda \lVert \vec{\beta} \rVert_1
$$

What is the difference? Notice the L2 norm is not squared (hence "SQRT-lasso")

> **Theorem.** SQRT-lasso and lasso have the same regularization path.

Why? (Han Liu says to think about this.)

### A robust optimization representation of lasso

> **Theorem.** The SQRT-lasso problem above is equivalent to the following robust linear regression problem:
> 
> $$
> \min_{\vec{\beta} \in \mathbb{R}^d} \{ \max_{\mat{U} \in \Omega \lambda} \lVert \vec{Y} - (\mat{X} + \mat{U}) \vec{\beta} \rVert_2 \}
> $$
>
> Where
>
> $$
> \Omega \lambda \coloneqq \{ \mat{U} \coloneqq (\vec{U_1}, ..., \vec{U_d}) \in \mathbb{R}^d, \max_{j} \lVert u_j \rVert_2 \leq \lambda \}
> $$
>
> Where $(\vec{U_1}, ..., \vec{U_d})$ are column vectors of $\mat{U}$.

#### Proof

<span style='color: red;'>CONFIRM: This proof is notationally dense. Confirm with actual notes.</span>

We only need to prove the following result:

$$
\max_{\mat{U} \in \Omega \lambda} \lVert \vec{Y} - (\mat{X} + \mat{U}) \vec{\beta} \rVert_2
=
\lVert \vec{\beta} - \mat{X} \vec{\beta} \rVert_2 + \lambda \lVert \vec{\beta} \rVert_1
$$

We show that the left hand side (LHS) is less than or equal to the right hand side (RHS) and vice versa.

**LHS:**

$$
\max_{\mat{U} \in \Omega \lambda} \lVert \vec{Y} - (\mat{X} + \mat{U}) \vec{\beta} \rVert_2
$$

$$
= \max_{\mat{U} \in \Omega \lambda} \lVert \vec{Y} - \mat{X} \vec{\beta} - \sum_{j=1}^{d} \beta_j \vec{U}_j \rVert_2
$$

<span style='color: red;'>WARNING: Something here seems wrong. $\sum_{j=1}^{d} \beta_j \vec{U}_j$ does not seem, at least to me, to be the same thing as $\mat{U} \vec{\beta}$.</span>

By the triangle inequality:

$$
\leq \max_{\mat{U} \in \Omega \lambda}
\lVert \vec{Y} - \mat{X} \vec{\beta} \rVert_2
+
\max_{\mat{U} \in \Omega \lambda}
\sum_{j=1}^{d} |\beta_j| \lVert \vec{U_j} \rVert_2
$$

$$
\leq \max_{\mat{U} \in \Omega \lambda}
\lVert \vec{Y} - \mat{X} \vec{\beta} \rVert_2
+
\lambda \sum_{j=1}^{d} |\beta_j| \equiv \text{RHS}
$$

**RHS:**

We define

$$
\vec{U} = \begin{cases}
	\vec{Y} - \mat{X} \vec{\beta} & \text{If } \vec{Y} \neq \mat{X} \vec{\beta} \\
	\lVert \vec{Y} = \mat{X} \vec{\beta} \rVert_2 \\
	\text{Any vector with L2 norm} & \text{Otherwise}
\end{cases}
$$

<span style='color: red;'>QUESTION: What's going on here? Why are there three options but only two conditions?</span>

$$
U_j^* = \begin{cases}
	- \lambda \text{sign}(\beta_j) \vec{U} & \text{If } \beta_j \neq 0 \\
	- \lambda \vec{U} & \text{Otherwise}
\end{cases}
$$

$$
\max_j \lVert U_j^* \rVert \leq \lambda \implies \mat{U}^* = (U_1^*, ..., U_d^*) \in \Omega \lambda
$$

$$
\text{LHS} = \max_{\mat{U} \in \Omega \lambda}
\lVert
\vec{Y} - (\mat{X} + \mat{U}) \vec{\beta}
\rVert_2
$$

$$
\leq 
\lVert 
\vec{Y} - \mat{X} \vec{\beta} 
- \sum_{j=1}^{d} \beta_j U_j^*
\rVert_2
$$

$$
= \lVert
\vec{Y} - \mat{X} \vec{\beta}
+ \lambda \sum_{j=1}^{d} \lVert \beta_j \rVert_1
\frac{\vec{Y} - \mat{X} \vec{\beta}}{\lVert \vec{Y} - \mat{X} \vec{\beta} \rVert_2}
\rVert_2
$$

$$
= \lVert
\vec{Y} - \mat{X} \vec{\beta}
\rVert_2
+ \lambda \lVert \vec{\beta} \rVert_1
\frac{\vec{Y} - \mat{X} \vec{\beta}}{\lVert \vec{Y} - \mat{X} \vec{\beta} \rVert_2}
$$

$$
= \lVert
\vec{Y} - \mat{X} \vec{\beta}
\rVert_2
+ \lambda \lVert \vec{\beta} \rVert_1
= \text{RHS}
$$

### Robustness

"Robust" against $\mat{U} \in \Omega \lambda$. Think about this.

The persistency theory of lasso:

We define

$$
R(\beta) = \mathbb{E}_{Y,\vec{X}} (Y - \vec{\beta}^{\top} \vec{X})^2
$$

Where $R$ is the population risk.

Then the sample (or empirical) risk $\hat{R}$ is:

$$
\hat{R}(\beta) = \frac{1}{n} \sum_{i=1}^{n} (Y_i - \vec{\beta}^{\top} \vec{X})^2
$$

<span style='color: red;'>This is just the average error we have seen so far.</span>

In these terms, the lasso estimate is:

$$
\hat{\vec{\beta}} \coloneqq \arg\min_{\lVert \vec{\beta} \rVert_1 \leq L} \hat{R}(\vec{\beta})
$$

Our _oracle estimator_ is:

$$
\vec{\beta^*} \coloneqq \arg\min_{\lVert \vec{\beta} \rVert_1 \leq L} \hat{R}(\vec{\beta})
$$

<span style='color: red;'>I believe by "oracle estimator", Han Liu just means that we can give some orcale a $\beta$ and it would output $\beta^*$.</span>

> **Definition.** An estimator $\hat{\vec{\beta}}$ is persistent within a class $\text{Bn}$ if we have:
>
> $$
> R(\hat{\vec{\beta}}) - \inf_{\vec{\beta} \in \text{Bn}} R(\vec{\beta}) \stackrel{\text{P}}{\rightarrow} 0
> $$
>
> As $n \rightarrow \infty$.

This is only impressive if it is persistent within a class.

> **Theorem.** Lasso is persistent.
>
> Assume $\|Y_i\| \leq B$ and $\max_i \lVert X_i \rVert_{\infty} \leq B$
>
> Then 
> 
> $$
> \mathbb{P}(R(\hat{\vec{\beta}}^{\text{Lasso}})) - \inf_{\lVert \vec{\beta} \rVert_1} R(\vec{\beta}) \leq 2
> $$
>
> $$
> \mathbb{P}(R(\hat{\vec{\beta}}^{\text{Lasso}})) - \inf_{\lVert \vec{\beta} \rVert_1 \leq L} \leq 2 (1 + L)^2 2 B^4 \sqrt{\frac{\log{\frac{2d^2}{\delta}}}{m}} \geq 1 - \delta
> $$
>
> For all $\delta \in (0,1)$

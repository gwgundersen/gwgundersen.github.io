---
title: Bridge and Lasso estimator
number: 7
layout: orf525
date: 2017-02-23
---

# Admin

- HW1 is assigned
    - It is rather theoretical
    - Start early and work backwards (start with the last problem)

# Review

- Statistical learning
	- Ordinary least squares
	- Basis expansion
	- Ridge estimator
	- Data splitting
	- Cross validation
- Deep learning
    - Fully connected NN
    - Convolutional NN
    - Recurrent NN
    - Stochastic gradient descent

# Bridge estimator

### (Review): ridge estimator

From [Metacademy](https://metacademy.org/graphs/concepts/ridge_regression):

> A problem with vanilla linear regression is that it can overfit, by forcing the learned parameters to match all the idiosyncrasies of the training data. Ridge regression, or regularized linear regression, is a way of extending the cost function with a regularizer which penalizes large weights. This leads to simpler solutions and often improves generalization performance. This idea of regularization can be used to improve the generalization performance of many other statistical models as well. 

$$
\hat{\vec{\beta}}^{\text{Ridge}} = \arg\min_{\vec{\beta}} = \lVert \vec{Y} - \vec{X} \vec{\beta} \rVert_2^2 + \lambda \lVert \vec{\beta} \rVert_2^2
$$

### Bridge estimator

$$
\vec{\beta}^{\text{Bridge}} = \arg\min_{\vec{\beta}} \lVert \vec{Y} = \vec{X} \vec{\beta} \rVert_2^2 + \lambda \lVert \vec{\beta} \rVert_p^p
$$

Here, we define:

$$
\lVert \vec{\beta} \rVert_p = (|\beta_1|^p, ..., |\beta_2|^p)^{\frac{1}{p}}
$$

If $p = 2$, we have the L2 norm.

#### Remark

- If $1 \leq p < \infty \implies \lVert \cdot \rVert_p$ is a norm.
- If $0 < p < 1 \implies \lVert \cdot \rVert_p$ is _not_ a norm.

#### Geometrically

<img src="{{ site.url }}/images/orf525/bridge-estimator.png" style="width: 700px;"/>

# Lasso estimator

- 1996
- A special case of the bridge estimator
- **L**east **A**bsolute **S**hrinkage and **S**election **O**perator

> **Definition.** The bridge estimator with $p = 1$:
>
> $$
> \hat{\vec{\beta}}^{\lambda}
> = \arg\min_{\vec{\beta}} \lVert \vec{Y} - \vec{X} \vec{\beta} \rVert_2^2 + \lVert \vec{\beta} \rVert_1
> $$

The above equation is in **regularization form**.

Why is the lasso nice?

- _Selection_ is what makes lasso especially different.
- Geometrically, lasso should converge to a fixed point since it is "pointy"
- _Shrinkage_: move the estimator close to 0

For all $\lambda$, there exists $t$ such that:

$$
\hat{\vec{\beta}}^{\lambda} = \arg \min_{\lVert \vec{\beta} \rVert_1 \leq t} \lVert \vec{Y} - \vec{X} \vec{\beta} \rVert_2^2
$$

The above equation is in **constraint form**.

<img src="{{ site.url }}/images/orf525/lasso-estimator.png" style="width: 450px;"/>

# Sparsity

- Many elements of $\vec{\beta}$ are 0.
- This is a kind of variable selection.
- For example

	$$
	f(\vec{x}) = \beta_1 x_1 + \beta_2 x_2 + ... + \beta_d x_d
	$$

	If $\beta_j = 0$, then the $j$th feature or variable $x_j$ is **not selected** since its coefficient is 0.

### In the bridge family:

- If $0 \leq p \leq 1$ then sparsity
	- <span style="color: red;">My intuition for this is that the constraint forces some $\beta_j$'s to be 0 because otherwise it would not "fit" in the hypercube.</span>
- If $1 \leq p \leq \infty$ then convexity
	- <span style="color: red;">See the drawing for when $p = 0.5$. It should be visually obvious that that plot is not convex.</span>
- With lasso, $p = 1$, so it is both sparse and convex!

# Ridge vs. Lasso

### Remark on collinearity

> **Definition.** _Collinearity_ is a phenomenon in which two or more predictors are highly correlated.

Remember our ridge estimator:

$$
\hat{\vec{\beta}}^{\lambda} = (\vec{X}^{\top} \vec{X} + \lambda \vec{I_d})^{-1} \vec{X}^{\top} \vec{Y}
$$

The term $\lambda \vec{I_d}$ handles collinearity well because no value on the diagonal can be less than $\lambda$.

(Also can say: the data is rank deficient.)

### Comparison

- Ridge
	- Not sparse
	- Handles collinearity
- Lasso
	- Sparse
	- Does _not_ handle collinearity

<img src="{{ site.url }}/images/orf525/lasso-collinearity.png" style="width: 400px;"/>

# Elastic net

**Idea:** combine advantages of lasso and ridge.

$$
\hat{\vec{\beta}}^{\text{Elastic}} = \arg \min_{\vec{\beta}}
\lVert \vec{Y} - \mat{X} \vec{\beta} \rVert
+
\lambda [\alpha \lVert \vec{\beta} \rVert_1 + (1 - \alpha) \lVert \vec{\beta} \rVert_2^2]
$$

In other words

- $\alpha = 1 \implies $ lasso
- $\alpha = 0 \implies $ ridge

Rule of thumb: $\alpha = 0.6$. Why? This works well in practice.
---
title: Lecture 7 - Bridge and Lasso estimator
layout: orf525
date: 2017-02-23
---

# Admin

- HW1 is assigned
    - It is rather theoretical
    - Start early and work backwards (start with the last problem)

# Review

- Statistical learning
	- Ordinary least squares
	- Basis expansion
	- Ridge estimator
	- Data splitting
	- Cross validation
- Deep learning
    - Fully connected NN
    - Convolutional NN
    - Recurrent NN
    - Stochastic gradient descent

# Ridge estimator

From [Metacademy](https://metacademy.org/graphs/concepts/ridge_regression):

> A problem with vanilla linear regression is that it can overfit, by forcing the learned parameters to match all the idiosyncrasies of the training data. Ridge regression, or regularized linear regression, is a way of extending the cost function with a regularizer which penalizes large weights. This leads to simpler solutions and often improves generalization performance. This idea of regularization can be used to improve the generalization performance of many other statistical models as well. 

$$
\hat{\beta}^{\text{Ridge}} = \arg\min_{\vec{\beta}} = \lVert \vec{Y} - \boldsymbol{X} \vec{\beta} \rVert_2^2 + \lambda \lVert \vec{\beta} \rVert_2^2
$$
---
title: Lecture 2
layout: orf525
date: 2017-02-09
---

# Lecture 2

### Last lecture

- Foundational principles
- Basic concepts
- Estimation
- Regression
- Overfitting / regularization

### Ordinary least squares (OLS)

<span style="color: red;">CONFIRM: Check this against the lecture notes.</span>

$$
\vec{Y} = \begin{bmatrix}
    Y_1 \\
    ... \\
    Y_n
\end{bmatrix}
$$

$$
X = \begin{bmatrix}
    X_{11} && ... && Y_{1d} \\
    ... && ... && ... \\
    Y_{n1} && ... && Y_{nd}
\end{bmatrix}
$$

$$
\vec{\beta} \in \mathbb{R}^d
$$

$$
\|\beta_2\| = \sqrt{\vec{B}^{\top} \vec{\beta}}
$$

OLS:

$$
\hat{\beta} = \arg \min \|\vec{Y} - X\vec{\beta}\|_2^2 = (X^{\top} X)^{-1} X^{\top} \vec{Y}
$$

#### Model-based interpretation of OLS

Statistical model:

$$
Y = \beta^{\top} \vec{X} + \epsilon
$$

Where $\epsilon$ is Gaussian noise.

$$
\epsilon \sim N(0, \sigma^2)
$$

$$
\mathbb{E}(\epsilon | \vec{X}) = 0
$$

- <span style="color: red;">QUESTION: Does this mean that noise, given the data, will eventually be negligible?</span>

#### Joint log likelihood

$$
l_n(\vec{\beta}, \sigma^2) = \sum_{i=1}^{n} \log P_{\vec{\beta}, \sigma^2} (Y_i, X_i)
$$

$$
= \sum_{i=1}^{n} \log_{\vec{\beta}, \sigma^2} P(Y_i|X_i) + \sum_{i=1}^{n} \log P(X_i)
$$

- This is just the definition of the joint probability and log manipulation:
    
    $$
    P(A,B) = P(A|B)P(B)
    $$

    $$
    \log(P(A,B) = \log(P(A|B)P(B)) = \log(P(A|B)) + \log(P(B))
    $$

- Notice that the second term does not depend on $\beta$ or $\sigma$ (<span style="color: red;">QUESTION: Why not?</span>).

$$
\implies \arg \max_{\vec{\beta}, \sigma^2} l_n(\vec{\beta}, \sigma^2) = \arg \max_{\vec{\beta}, \sigma^2} \sum_{i=1}^{n} \log P_{\vec{\beta}, \sigma^2} (Y_i | X_i)
$$

$$
\arg \min_{\vec{\beta}, \sigma^2} \frac{1}{2} \sum_{i=1}^{n} \frac{(Y_i - \vec{\beta}^{\top}\vec{X})^2}{\sigma^2} + n \log(\frac{1}{2 \pi \sigma^2})
$$

$$
\hat{\beta}^{MLE} = \arg \min_{\beta} \sum_{i=1}^{n} (Y_i - \beta^{\top} X_i)^2
$$

<span style="color: red;">QUESTION: I do not follow the last two steps.</span>


## Linear regression with basis expansion

From linear to nonlinear:

1. **Input can be transformations of original features.**

    _Examples:_ 

    - $x_i = \log(x_i)$
    - $x_i = \sqrt{x_i}$
    - $x_i = x_i^2$

    These are called "hand-crafted features".

2. **Input can have interaction terms.**

    $$
    x_1, x_2; x_2, x_3; ... x_{d-1}, x_d
    $$

    <span style="color: red;">CONFIRM: Notation.</span>

3. **Inputs can have basis expansions.**

    - Instead of using $f(\vec{x}) = \sum_{j=1}^{d} \beta_j x_j$, we consider:

    $$
    f(\vec{x}) = \sum_{j=1}^{d} \beta_j h_j (\vec{x})
    $$

    <span style="color: red;">QUESTION: What is $h_j$?</span>

4. **Indicator functions of qualitative inputs.**

    - Let $\mathbb{I}$ be the [indicator function](https://en.wikipedia.org/wiki/Indicator_function).

    $$
    \mathbb{I}(x_j = condition)
    $$

    - For example, we can condition on categorial features such as cities, that have no meaning in the data but might to us.
    - More generally, categorial data analysis.


#### Categorical variables

> **Definition** of _Categorial categorial_: A variable that can take on one of a limited set of values.

- Categorical feature: dummy coding
- Dummy coding: if categorical variable has $K$ categories:
    - City 1: $0, 0, ..., 0, 0$
    - City 2: $1, 0, ..., 0, 0$
    - ...
    - City K: $0, 0, ..., 0, 1$
    - In other words, ($K-1$)-dimensional one-hot arrays


## High-dimensional data

> **Definition** of _High-dimensional data: Intuitively, "a lot of" features. Specifically, when $d$ is comparable or larger than the sample space $n$.

_Example_: Recall OLS:

$$
\hat{\beta}^{OLS} = (X^{\top} X)^{-1} X^{\top} T
$$

The term $(X^{\top} X)^{-1}$ is $d \times d$ in shape. If $d > n$, then the matrix is not invertible. Or "rank deficient." We need to regularize.

### Ridge estimator

From [Wikipedia](https://en.wikipedia.org/wiki/Tikhonov_regularization):

> Suppose that for a known matrix $\textbf{A}$ and vector $\vec{b}$, we wish to find a vector $\vec{x}$ such that
>
> $$
> \textbf{A} \vec{x} = \vec{b}
> $$
>
> The standard approach is ordinary least squares linear regression. However, if no $\vec{x}$ satisfies the equation or more than one $\vec{x}$ does — that is, the solution is not unique — the problem is said not to be well posed. In such cases, ordinary least squares estimation leads to an overdetermined (over-fitted), or more often an underdetermined (under-fitted) system of equations.

$$
\hat{\beta}^{\lambda} = (X^{\top} X + \lambda I_d)^{-1} X^Y
$$

Where $I_d$ is a $d$-dimensional identity matrix.

The smallest value of $X^{\top} X + \lambda I_d$ is at least $\lambda$, i.e. it's _stable_.

$$
\Leftrightarrow \hat{\beta}^{\lambda} = \arg \min_{\vec{\beta}} \|\vec{Y} - X\vec{B}\|_2^2 + \lambda \|\vec{\beta}\|_2^2 
$$

$$
\Leftrightarrow \hat{\beta}^{\lambda} = \arg \min_{\|\vec{\beta}\|_2^2 \leq t} \|\vec{Y} - X\vec{B}\|_2^2
$$


### Lemma

$\forall \lambda > 0$, $\exists t$ s.t. $\exists$ a one-to-one-mapping:

$$
\hat{\beta}^{\lambda} = \arg \min_{\|\hat{\beta}\|_2^2 \leq t} \|\vec{Y} - X \vec{B}\|_2^2
$$

**Remark**: When $t$ is big enough, s.t. 

$$t > \| \hat{\beta}^{OLS} \|_2^2$$

then this corresponds to $\lambda = 0$ if $t = 0 \implies \lambda = \infty$.



### Geometry of ridge estimator

$$
d = 2
$$

$$
\vec{\beta} = \begin{bmatrix} 
    \beta_1 \\
    \beta_2
\end{bmatrix}
$$

<img src="{{ site.url }}/images/orf525/ridge_estimator_diagram.png" alt="Plot of racecar speed as a function of time." style="width: 500px;"/>

Contours are from:

$$
F(\vec{\beta}) = \| \vec{Y} - X \vec{\beta} \|_2^2
$$

Lots of algorithms to solve this, since it is convex.


## Model selection

Each $\lambda$ indexes a model.

Basic strategy: data splitting. We split the data $D = [x_1, ..., x_n]$ into two subsets, $D_1$ and $D_2$ such that:

$$
D_1 \cup D_2 = D
$$

And if $n = size(D)$, $n_1 = size(D_1)$, $n_2 = size(D_2)$:

$$
n_1 + n_2 = n
$$

Consider a pool of tuning parameters:

$$
\Lambda = [\lambda_1, ..., \lambda_k]
$$

Let $\hat{\beta}^{\lambda_1}, ..., \hat{\beta}^{\lambda_k}$ be ridge estimators using $\lambda_1, ..., \lambda_k$ on $D_1$. We define the data splitting score corresponding to $\lambda_k$:

$$
DS(k) = \frac{1}{n_2} \sum_{i \in D_2} (Y_i - \vec{X_i} \hat{\beta}^{\lambda^k})^2
$$

In words, it is how well that ridge estimator parameterized by $\lambda_k$ approximates the data.

**Theory:** Conditioned on $D_1$, it is easy to see that $DS(k)$ is an unbiased estimator of $R(\hat{\beta^{\lambda_k}})$, where $R$ is the risk. 

<span style="color: red;">QUESTION: Empirical risk?</span>

### Pros and cons of data splitting

- **Pros:** Theoretically and computationally simple.

- **Cons:** "Waste" some of the training data.

_Solution_: Cross-validation (CV)


### Cross-validation (especially J-fold CV)

GWG: This is more commonly referred to as [_k-fold cross-validation_](https://en.wikipedia.org/wiki/Cross-validation_(statistics)#k-fold_cross-validation), but I will be consistent with Prof. Liu's notes.

> **Definition** of _J-fold CV_: Split data $D$ into $J$ equally sized subsets, $D_1, ..., D_J$. This forms $J$ binary splits:
>
> - $DS_1$ = $D_1$ vs. $D$ \ $D_1$
> - $DS_2$ = $D_1$ vs. $D$ \ $D_2$
>
> ...
>
> - $DS_j$ = $D_1$ vs. $D$ \ $D_J$
>
> For $\lambda_k \in \Lambda$, we calculate the data splitting score using subsets $DS_1, ..., DS_j$. Denote the results as $DS_1(k), ..., DS_j(k)$. The CV score is:
>
> $$
> CV(k) = \frac{1}{j} \sum_{j=1}^{J} DS_j(k)
> $$
>
> Take the model with the smallest score.


#### "One standard deviation rule"

Sum curves to find $\lambda_k$ that minimizes curve:

<img src="{{ site.url }}/images/orf525/sum_of_j_curves.png" alt="Plot of racecar speed as a function of time." style="width: 500px;"/>

$SD(\lambda)$: Calculate the standard deviation out of these $J$ CV values for each $\lambda$.

> **Rule:** Pick the most parsimonious model where the error is no more than one standard deviation above model.


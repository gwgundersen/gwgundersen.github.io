---
title: Lecture 2
layout: default
---

# Lecture 2

### Last lecture

- Foundational principles
- Basic concepts
- Estimation
- Regression
- Overfitting / regularization

### Ordinary least squares (OLS)

$$
\vec{Y} = \begin{bmatrix}
    Y_1 \\
    ...
    Y_n
\end{bmatrix}
$$

$$
X = \begin{bmatrix}
    X_{11} && ... && Y_{1d} \\
    ... && ... && ... \\
    Y_{n1} && ... && Y_{nd}
\end{bmatrix}
$$

$$
\vec{\beta} \in \mathbb{R}^d
$$

$$
|\beta_2| = \sqrt{\vec{B}^{\top} \vec{\beta}}
$$

OLS:

$$
\hat{\beta} = \arg \min |\vec{Y} - X\vec{\beta}|^2 = (X^{\top} X)^{-1} X^{\top} \vec{Y}
$$

#### Model-based interpretation of OLS

Statistical model:

$$
Y = \beta^{\top} \vec{X} + \epsilon
$$

Where $\epsilon$ is Gaussian noise.

$$
\epsilon \sim N(O, \sigma^2)
$$

$$
\mathbb{E}(\epsilon | \vec{X}) = 0
$$

#### Joint log likelihood

$$
l_n(\vec{\beta}, \sigma^2) = \sum_{i=1}^{n} \log P_{\beta, \sigma^2} (Y_i, X_i)
$$

$$
= \sum_{i=1}^{n} \log_{\beta, \sigma^2} (Y_i|X_i) + \sum_{i=1}^{n} \log P(x_i)
$$

- Notice that the second term does not depend on $\beta$ or $\sigma$.

$$
\implies \arg \max_{\beta, \sigma^2} l_n(\vec{B}, \sigma^2) = \arg \max_{\beta, \sigma^2} \sum_{i=1}^{n} \log P_{\beta, \sigma^2} (Y_i | X_i)
$$

$$
\arg \min_{\beta, \sigma^2} \frac{1}{2} \sum_{i=1}^{n} \frac{(Y_i - \vec{\beta}^{\top}\vec{X})^2}{\sigma^2} + n \log(\frac{1}{2 \pi \sigma^2})
$$

$$
\hat{\beta}^{MLE} = \arg \min_{\beta} \sum_{i=1}^{n} (Y_i - \beta^{\top} X_i)^2
$$


## Linear regression with basis expansion

From linear to nonlinear:

1. **Input can be transofrmations of original features.**

    _Examples:_ 

    - x_i = \log(x_i)
    - x_i = \sqrt(x_i)
    - x_i = x_i^2

    These are called "hand-crafted features".

2. **Input can have interaction terms.**

    $$
    x_1, x_2; x_2, x_3; ... x_{d-1}, x_d
    $$

3. **Inputs can have basis expansions.**

    Instead of using $f(\vec{x}) = \sum_{j=1}^{d} \beta_j x_j$, we consider:

    $$
    f(\vec{x}) = \sum_{j=1}^{d} \beta_j h_j (\vec{x})
    $$

4. **Indicator functions of qualitative inputs.**

    $$
    \mathbb{I}(x_j = condition)
    $$

    - For example, we can condition on categorial features such as cities, that have no meaning in the data but might to us.
    - More generally, categorial data analysis.


#### Categorical variables

> **Definition** of _Categorial categorial_: A variable that can take on one of a limited set of values.

- Categorical feature: dummy coding
- Dummy coding: if categorical variable has $K$ categories:
    - City 1: $0, 0, ..., 0, 0$
    - City 2: $1, 0, ..., 0, 0$
    - ...
    - City K: $0, 0, ..., 0, 1$
    - In other words, ($K-1$)-dimensional one-hot arrays


## High-dimensional data

> **Definition** of _High-dimensional data: Intuitively, "a lot of" features. Specifically, when $d$ is comparable or larger than the sample space $n$.

_Example_: Recall OLS:

$$
\hat{\beta}^{OLS} = (X^{\top} X)^{-1} X^{\top} T
$$

The term $(X^{\top} X)^{-1}$ is $d \times d$ in shape. If $d > n$, then the matrix is not invertible. Or "rank deficient." We need to regularize.


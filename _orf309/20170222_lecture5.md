---
title: Random variables and expectation
number: 5
layout: orf309
date: 2017-02-22
---

# Random variables

- A _random variable_ is a quantity whose value we can determine after an experiment.
- They are particularly useful in comparison to events, which can only be true or false.

> **Definition.**
>
> - $\Omega = \text{Sample space}$
> - $D = \text{Space of outcomes}$
> - $X: \Omega \rightarrow D$ is called a $D$-valued random variable.

### Example: flip 3 coins

Setup:

- $\Omega = \\{HHH, HHT, HTH, THH, TTH, THT, HTT, TTT\\}$
- $X = \text{Number of heads}$

Calculations:

- $X(HHH) = 3$
- $X(TTT) = 0$
- $X(TTH) = 1$

Analysis

- So what is $D$? Ideally, $D = \{0,1,2,3\}$ but it could be $\mathbb{R}$ without contradiction.
- $X$ is a $D$-valued random variable, and $x \in D$.
- Here is a more compact notation:

$$
	\{\omega \in \Omega : X(\omega) = x\} \iff \{X = x\}
$$

- So we will write $$\mathbb{P}(\{X = x\})$$ instead of $$\mathbb{P}(\{\omega \in \Omega : X(\omega) = x\})$$


### Example: flip repeatedly (independently) a coin with probability $p$

Setup

- $\Omega = \\{(\omega_1, \omega_2, ...), \omega_i \in \\{H,T\\}\\}$
- $E_i = \\{\text{Event that $i$th flip is heads}\\} = \\{\omega \in \Omega : w_i = H\\}$

Modeling assumption

- $E_i$s are independent, so $\mathbb{P}(\\{E_i\\}) = p$

Random variable

- Let $X$ denote the first time I flip heads.
- So $X$ is a random variable.
- $X: \Omega \rightarrow D$
- $D = \\{1,2,...\\} \cup \\{\infty\\}$
    - We add $\infty$ since we may never flip a heads.

Computations

#### Probability that $X = 5$?

$$
\mathbb{P}(\{X = 5\})?
$$

$$
\{X=5\} = \{E_1^c \cap E_2^c \cap E_3^c \cap E_4^c \cap E_5 \}
$$

$$
= \{\omega \in \Omega : \omega = (TTTTH...)\}
$$

Where "..." can be $H$ or $T$.

$$
\stackrel{\text{Indep.}}{=} \mathbb{P}(\{E_1^c\}) + \mathbb{P}(\{E_2^c\}) + \mathbb{P}(\{E_3^c\}) + \mathbb{P}(\{E_4^c\} + \mathbb{P}(\{E_5 \})
$$

$$
= (1 - p)^{(n-1)} p
$$

#### Probability that $X > 5$?

$$
\mathbb{P}(\{X > 5\})?
$$

##### First solution

$$
\{X > 5\} = \{\omega \in \Omega, \omega = (TTTTT...)\}
$$

Where "..." can be $H$ or $T$.

$$
E_1^c \cap ... \cap E_5^c
$$

$$
\mathbb{P}(\{E > 5\}) = (1 - p)^5
$$

##### Second solution

$$
\{X > 5\} = \bigcup\limits_{n=6}^{\infty} \{X=n\}
$$

$$
\mathbb{P}(\{X > 5\}) 
\stackrel{\text{Disj.}}{=} \sum_{\infty}^{n=6} \mathbb{P}(\{X=n\})
=
\sum_{\infty}^{n=6} (1 - p)^{(n-1)} p
$$

You can verify that the following is true (comparing the two solutions):

$$
(1 - p)^5 = \sum_{\infty}^{n=6} (1 - p)^{(n-1)} p
$$

#### Probability that $X \leq 5$?

##### First solution

$$
\mathbb{P}(\{X \leq 5\}) 
= \mathbb{P}(\{X=1\}) + \mathbb{P}(\{X=2\}) + ... + \mathbb{P}(\{X=5\})
= p + p(1 - p) + p(1 - p)^2 + ... + p(1 - p)^4
$$

##### Second solution

$$
\{X \leq 5\} = \{X > 5\}^c
$$

$$
\mathbb{P}(\{X \leq 5\}) = 1 - \mathbb{P}(\{X > 5\}^c) = 1 - (1 - p)^5
$$

Again, you can verify that the two solutions are the same.


# Expectation

_Intuition:_ If $X$ is a **numerical** random variable (i.e. $X: \Omega \rightarrow D$ with $D \subseteq \mathbb{R}$), the **expectation** of $X$ is the average outcome over many experiments.

Ramon: "Expectation" is a bad name. You cannot reason about the "expectation" of a random variable, since it is random. A better name is just "average".

For $x_1, x_2, ..., x_n$ outcomes in experiments $1, 2, ..., n$, for each $x_i \in D$:

$$
\frac{1}{n} \sum_{n}^{k=1} x_i
$$

#### Example

Let $D = \{1,2,3\} \subseteq \mathbb{R}$. Then:

$$
\frac{1}{n}(1 \cdot |\{i : x_i=1\}| + 2 \cdot |\{i : x_i=2\}| + 3 \cdot |\{i : x_i=3\}|)
$$

### Discrete random variable

> **Definition.** $X$ is a **discrete random variable** if $D$ is finite or countable:
>
> $$
> \frac{1}{n} \sum_{k=1}^{n} x_i = \sum_{x \in D} x \cdot \frac{|\{i: x_i = x\}|}{n}
> $$

Notice the term $\frac{\|\{i: x_i = x\}\|}{n}$ is just the fraction of experiments with outcome $x$.

> **Definition.** If $X$ is a discrete, numerical random variable, the **expectation of $X$** is:
>
> $$
> \mathbb{E}(X) = \sum_{x \in D} x \cdot \mathbb{P}(\{X=x\})
> $$


#### Example: daw a card from a deck

Setup

- $X = \text{Suit of card}$
- $X : \Omega \rightarrow D$
- $D = \\{♠, ♥, ♦, ♣\\}$
- Game 
	- If suit is ♥, ♦, get $1
	- If suit is ♠, lose $1
	- If suit is ♣, lose $10
- $\mathbb{E}(X) = $ ?
    - Ramon: "I hope you see this is a stupid idea."
    - Why? $D$ must be _numerical_.

Solution to the numerical problem

- Introduce a random variable $\phi$
- $\phi : D \rightarrow \mathbb{R}$
- $\phi(♣) = -10$
- $\phi(♥) = \phi(♦) = 1$
- $\phi(♠) = -1$

Now we can ask:

- $\mathbb{E}[\phi(x)] = $ ?

$$
\mathbb{E}[\phi(x)] =
\phi(♣) \mathbb{P}(\{x = ♣\}) +
\phi(♥) \mathbb{P}(\{x = ♥\}) +
\phi(♦) \mathbb{P}(\{x = ♦\}) +
\phi(♠) \mathbb{P}(\{x = ♠\})
$$

$$
= -10 \mathbb{P}(\{\phi(x) = -10\}) +
1 \mathbb{P}(\{\phi(x) = 1\}) +
-1 \mathbb{P}(\{\phi(x) = -1\})
$$

### More general definition

Now: suppose $X$ is a $D$-valued random variable and $\phi: D \rightarrow \mathbb{R}$. Then:

$$
\mathbb{E}[\phi(x)] = \sum_{x \in D} \phi(x) \mathbb{P}(\{X=x\})
$$

Remember: so far we have only looked at **discrete** random variables. $\mathbb{P}(\\{X=x\\}) = 0$ for **continuous** random variables. So we need a separate definition. We'll discuss this in a future lecture.
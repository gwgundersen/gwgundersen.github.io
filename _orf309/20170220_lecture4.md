---
title: Lecture 4 - Bayes' formula and independence
layout: orf309
date: 2017-02-20
---

# Bayes' formula

Last time, we defined conditional probability:

$$
    \mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
$$

We can rewrite as:


$$
    \mathbb{P}(A|B)\mathbb{P}(B) = \mathbb{P}(A \cap B)
$$

Observe the same is true of the complement:

$$
    \mathbb{P}(A|B^{C})\mathbb{P}(B^{C}) = \mathbb{P}(A \cap B^{C})
$$

Notice that the following is true:

<img src="{{ site.url }}/images/orf309/completeness.png" style="width: 300px;"/>

$$
    \mathbb{P}(A \cap B) + \mathbb{P}(A \cap B^{C}) = \mathbb{P}(A)
$$

$$
    \mathbb{P}(A|B)\mathbb{P}(B) + \mathbb{P}(A|B^{C})\mathbb{P}(B^{C}) = \mathbb{P}(A)
$$

Since $\mathbb{P}(A\|B)\mathbb{P}(B) = \mathbb{P}(A \cap B) = \mathbb{P}(B\|A)\mathbb{P}(A)$:

$$
    \mathbb{P}(B|A) = \frac{\mathbb{P}(A|B)\mathbb{P}(B)}{\mathbb{P}(A)}
$$

Substituting for $\mathbb{P}(A)$, we get Bayes' formula:

$$
    \mathbb{P}(B|A) = \frac
    {\mathbb{P}(A|B)\mathbb{P}(B)}
    {\mathbb{P}(A|B)\mathbb{P}(B) + \mathbb{P}(A|B^{C})\mathbb{P}(B^{C})}
$$

### Example: medical test

- If you have a disease, you test positive with 95% probability.
- If you don't have a disease, you test positive with 2% probability.
- About 1 in 1000 people have a disease.
- The test comes back positive. What is the probability that you have a disease?

Mathematical formulation:

- $B = \text{Have disease}$
- $A = \text{Event test is positive}$
- $\mathbb{P}(B) = \frac{1}{1000} = 0.001$
- $\mathbb{P}(A\|B) = 0.25$
- $\mathbb{P}(A\|B^{C}) = 0.02$

Ramon: Always start by giving everything a mathematical name. Write down what is asked for. Only then can we use probability theory.

$$
\mathbb{P}(B|A) = \frac{0.95 \times 0.001}{0.95 \times 0.001 + 0.02 \times 0.999} \approx 0.045
$$

So if the test is positive, there is only a small chance that you have the disease. Why this counter intuitive notion? Because it is much more likely to have a false positive (2%) than it is that you have a disease (0.1%). This is why we must do formal computations.

**Remark:** We didn't write down the sample space. Why not? It's not really important to solve our problem. You can reverse engineer the sample space if you'd like. Although in the future, sometimes even that may not be possible.


# Independence

> **Definition.** Events $A$, $B$ are independent if:
>
> $$
> \mathbb{P}(A|B) = \mathbb{P}(A)
> $$
>
> Or equivalently:
>
> $$
> \mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)} \implies \mathbb{P}(A)\mathbb{P}(B) = \mathbb{P}(A \cap B)
> $$

The second definition is what is often taught. Ramon finds the first definition more intuition. In English, it says that knowing about event $B$ tells us nothing about event $A$.

### Example: biased coin

- Flip a biased coin with probability heads $\theta$.
- $A = \{ \text{First coin is heads} \}$
- $B = \{ \text{Second coin is heads} \}$
- $\mathbb{P}(A \cap B) \stackrel{\text{Indep.}}{=} \mathbb{P}(A)\mathbb{P}(B)$

Question

- What is $\mathbb{P}(A \cap B^{C})$?

Solution

- Intuition dictates that $\mathbb{P}(A \| B^{C}) = \mathbb{P}(A)$.
- But notice the definition does not say that.
- Its true, but we need to check using the rules of probability.

$$
    \mathbb{P}(A) \stackrel{\text{Disjoint}}{=} \mathbb{P}(A \cap B) + \mathbb{P}(A \cap B^{C})
$$

$$
    \mathbb{P}(A \cap B^{C}) = \mathbb{P}(A) - \mathbb{P}(A \cap B)
$$

$$
    \stackrel{\text{Indep.}}{=} \mathbb{P}(A) - \mathbb{P}(A)\mathbb{P}(B)
$$

$$
    = \mathbb{P}(A)(1 - \mathbb{P}(B))
$$

$$
    \stackrel{\text{Def.}}{=} \mathbb{P}(A)\mathbb{P}(B^{C})
$$

- Visually:

[DRAWING]

- So we can now say the following:

$$
    \mathbb{P}(A \cap B^{C}) = \mathbb{P}(A)\mathbb{P}(B^{C}) = \theta (1 - \theta)
$$

$$
    \mathbb{P}(A^{C} \cap B) = \mathbb{P}(A^{C})\mathbb{P}(B) = (1 - \theta) \theta
$$

$$
    \mathbb{P}(A^{C} \cap B^{C}) = \mathbb{P}(A^{C}) \mathbb{P}(B^{C}) = (1 - \theta)^2
$$


#### Reverse engineering the sample space

(We do not have to do this every time. Ramon just wants to show that it can be done.)

- Model: two coins, probability $\theta$ it comes up heads
- What is the sample space?

$$
\Omega = \{HH, HT, TH, TT\}
$$

$$
A = \{HH, HT\}
$$

$$
B = \{HH, TH\}
$$

$$
\mathbb{P}(\{HH\}) = \theta^2
$$

$$
\mathbb{P}(\{HT\}) = \theta (1 - \theta)
$$

$$
...
$$

$$
\mathbb{P}(A) = \sum_{\omega \in A} \mathbb{P}(\{\omega\})
$$


### Independence more generally

> **Definition.** Events $E_1, ..., E_n$ are independent if:
>
> $$
> \mathbb{P}(E_{i_1} | E_{i_2} \cap ... \cap E_{i_k}) = \mathbb{P}(E_{i_1})
> $$
>
> For $2 \leq k \leq n-1$ and for all $1 \leq i_1 + i_2 + ... + i_k \leq n$

A direct generalization is:

$$
    \mathbb{P}(E_{i_1} \cap ... \cap E_{i_k}) =
$$

$$
    = \mathbb{P}(E_{i_1} | E_{i_2} \cap ... \cap E_{i_k}) \mathbb{P}(E_{i_2} \cap ... \cap E_{i_k})
$$

$$
    = \mathbb{P}(E_{i_1}) \mathbb{P}(E_{i_2} \cap ... \cap E_{i_k})
$$

$$
    ...
$$

$$
    = \mathbb{P}(E_{i_1}) \mathbb{P}(E_{i_2}) ... \mathbb{P}(E_{i_k})
$$

#### Example: flip 5 coins independently

- Each coin has probability $\theta$ of coming up heads
- Question: What is probability of 1 tails and 4 heads?

$$
    E_i = \{ \text{$i$th coin is heads} \}
$$

$$
    \mathbb{P}(E_i) = \theta
$$

$$
    A = \{T, H, H, H, H\}
$$

$$
    = 
    (E_1^{C} \cap  E_2 \cap ... \cap  E_5)
    \cup
    (E_1 \cap  E_2^{C} \cap ... \cap  E_5)
    \cup
    ...
    \cup
    (E_1 \cap  E_2 \cap ... \cap  E_5^{C})
$$

$$
    \mathbb{P}(A)
    \stackrel{Mut. exec.}{=}
    \mathbb{P}(E_1^{C} \cap  E_2 \cap ... \cap  E_5)
    +
    \mathbb{P}(E_1 \cap  E_2^{C} \cap ... \cap  E_5)
    +
    ...
    +
    \mathbb{P}(E_1 \cap  E_2 \cap ... \cap  E_5^{C})
$$

$$
    = 5 (1 - \theta) \theta^4
$$

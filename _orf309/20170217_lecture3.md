---
title: Probabilistic modeling
number: 3
layout: orf309
date: 2017-02-17
---

# Reminder:

When modeling a random experiment:

- Write down all possible outcomes $\Omega$ or _sample space_
- $A \subseteq \Omega$ is an event $A$
- $\mathbb{P}(A)$ is the probability of an event, a number
    - $0 \leq \mathbb{P}(A) \leq 1$
    - $\mathbb{P}(\Omega) = 1$
    - If $A_1, A_2, ...$ are disjoint, $\mathbb{P}(\bigcup\limits_{i=1}^{\infty} A_i) = \sum_{i=1}^{\infty} \mathbb{P}(A_i)$

Ramon: "This is language." How we talk about the random experiment.

# Probabilistic modeling

- How do we choose $\mathbb{P}$ in a given situation?
    - A combination of our modeling assumption and experimental results
    - The entire goal of **statistics** is here.
    - The mathematical field of **probability** is what happens once you have $\mathbb{P}$

### Example: throw a die

$$\Omega = \{1,2,3,4,5,6\}$$

How do we assign $\mathbb{P}$? We make a **modeling assumption**: "Every outcome is equally likely."

This heavily constrains what $\mathbb{P}$ can be:

- $\mathbb{P}(\{1\}) = \mathbb{P}(\{2\}) = ... = \mathbb{P}(\{6\}) = x$
- How we just compute each probability

    $$
    \mathbb{P}(\Omega) = 1 = \mathbb{P}(\bigcup\limits_{\omega=1}^{6} \{\omega\}) = \sum_{\omega = 1}^{6} \mathbb{P}(\{\omega\}) = 6x
    $$

- So $x = \frac{1}{6}$
- In general, for any event $A$:

    $$
    \mathbb{P}(A) = \mathbb{P}(\bigcup\limits_{\omega \in A} \{\omega\}) = \sum_{\omega \in A} \mathbb{P}(\{\omega\}) = \frac{|A|}{6}
    $$

### Example: waiting for a bus

**Modeling assumption:** The bus comes in 1 hour, but it is equally likely to come at any time in this hour.

- $\Omega = [0,1]$
- Imagine we have two events $A, B$ that the bus comes at between $\epsilon$ and $\epsilon + t$ and time $\delta$ and $\delta + t$.

- [DRAWING TODO]

- These events capture the same length of time.
- So our modeling assumption suggests: $\mathbb{P}(A) = \text{length($A$)}$
- Let's verify that this probability measure satisfies Kolmogorov's axioms.
    - $A = \text{Bus comes exactly after 30 minutes} = \{\frac{1}{2}\}$
    - $\mathbb{P}(A) = 0$. In general, $\mathbb{P}(\{\omega\}) = 0$. This is why we have to specify $\mathbb{P}$ for **events** and not **outcomes**:

    $$
    A = \bigcup\limits_{\omega \in A} \{\omega\} = \mathbb{P}(A) = \mathbb{P}(\bigcup\limits_{\omega \in A} \{\omega\})) = \sum_{\omega \in A} \mathbb{P}(\{\omega\}) = 0
    $$

    - So what is the problem? Real numbers are not countable. This is subtle. Notice the axioms specify **countable events**.

# Conditional probability

- Let's modify the scenario of waiting for the bus. Now imagine that we know for certain that the bus will come in the first 10 minutes. Now the probability that it comes within the first 5 minutes _increases_.

- [DRAWING TODO]

- So once we know that the bus will come in 10 minutes, we can update the probability that it comes in 10 minutes to $\frac{1}{2}$

> **Definition**. The _conditional probability_ of an event $A$ given an event $B$ is defined as:
>
> $$
> \mathbb{P}(A|B) = \frac{\mathbb{P}(A \cap B)}{\mathbb{P}(B)}
> $$
>
> If $\mathbb{P}(B) > 0$.

- (We can prove this later using the law of large numbers.)
- The above definition is pretty intuition if you think about it: repeat our random experiment many times and only look at those experiments for which the event $B$ happens. What if $B$ does not happen? Discard that experiment. Then we just count the fraction of times $A$ and $B$ both happened over the total time $B$ happened.

### Does this behave like a probability measure?

1. $0 \leq \mathbb{P}(A\|B) \leq 1$
    - $0 \leq$ since the ratio of two nonnegative numbers is nonnegative
    - $\leq 1$ since $A \cap B$ can never be greater than $B$.
2. $\mathbb{P}(\Omega \| B) = 1$ since $\Omega \cap B = B$.
3. If $A_1, A_2, ...$ are disjoint:

    $$
    \mathbb{P}(A_1 \cup A_2 | B) = \frac{\mathbb{P}(A_1 \cup A_2 | B)}{\mathbb{P}(B)}
    $$

    $$
    = \frac{\mathbb{P}(A_1 \cap B) \cup \mathbb{P}(A_2 \cap B)}{\mathbb{P}(B)}
    $$

    $$
    = \frac{\mathbb{P}(A_1 \cap B)}{\mathbb{P}(B)} + \frac{\mathbb{P}(A_2 \cap B)}{\mathbb{P}(B)}
    $$

    $$
    \mathbb{P}(A_1 | B) + \mathbb{P}(A_2 | B)
    $$

    - We can see this visually as well:

    - [DRAWING TODO]



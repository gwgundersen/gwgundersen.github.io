---
title: Expectation and distributions
number: 6
layout: orf309
date: 2017-02-24
---

# Review

### Expectation

$X: \Omega \rightarrow D$ is a discrete random variable.

$\phi: D \rightarrow \mathbb{R}$ (This maps $D$ to a numerical value).

$$
\mathbb{E}[\phi(x)] = \sum_{x \in D} \phi(x) \mathbb{P})(\{X=x\})
$$

# Distributions

> **Definition.** The **distribution** of a discrete random variable $X: \Omega \rightarrow D$ is given by $(\mathbb{P}(\\{X=x\\}))_{x \in D}$

The distribution is the information you need to compute all possible statistics $\mathbb{E}[\phi(x)]$ for all $\phi$.

> **Definition.** If $D$ is a finite set, a random variable such that:
>
> $$
> \mathbb{P}(\{X=x\}) = \frac{1}{|D|}
> $$
>
> For all $x \in D$ is said to be **uniformly distributed** on $D$.
>
> This is equivalent to saying it has the **uniform distribution**.

### Joint distribution

Suppose $X$, $Y$ are both uniformly distributed on $\\{0,1\\}$.

- $\mathbb{P}(\{X=0\}) = \frac{1}{2}$
- $\mathbb{P}(\{Y=0\}) = \frac{1}{2}$
- $\mathbb{P}(\{X=1\}) = \frac{1}{2}$
- $\mathbb{P}(\{Y=1\}) = \frac{1}{2}$

Question: $\mathbb{P}(\\{X=0, Y=1\\}) =$ ?

Answer: we _cannot know this_ from the probabilities. It could be anything. For example, imagine that $X$ and $Y$ are independent vs. dependent. Depending on that assumption, we would have a different joint probability.

What information do we need to compute:

$$
\mathbb{E}[\phi(X,Y)]
$$

For discrete random variables $X$, $Y$ where $X \in D$ and $Y \in D^{\prime}$.

We can define $Z = (X,Y)$ as a new random variable such that:

$$
Z \in \{(x,y), x \in D, y \in D^{\prime}\}
$$

So:

$$
\mathbb{E}[\phi(X, Y)]
= \mathbb{E}[\phi(Z)]
= \sum_{x \in D, y \in D^{\prime}} \phi(x, y) \mathbb{P}(\{X=x, Y=y\})
$$

This is the **joint distribution**.

> **Definition.** The **joint distribution** of a discrete random variable $X: \Omega \rightarrow D$ and $Y: \Omega \rightarrow D^{\prime}$ is given by:
>
> $$
> (\mathbb{P}(\{X=x, Y=y\}))_{x \in D, y \in D^{\prime}}
> $$
>
> We can do this with any number of random variables.

The above contains the information you need to compute $\mathbb{E}[\phi(X,Y)]$ for all $\phi$.

#### Remark

1. Knowing the distribution of $X$ and the distribution of $Y$ is _not_ enough information to specify the joint distribution of $X$ and $Y$.

2. Knowing the joint distribution of $X$, $Y$ should be enough informatoin to compute the distribution of $X$ and the distribution of $Y$. Just define a function $\phi(X,Y)$ that only depends on $X$.


### Marginal distribution

Point #2 above is called the **marginal distribution**:

$$
\mathbb{P}(\{X=x\}) = \sum_{y \in D^{\prime}} \mathbb{P}(\{X=x, Y=y\})
$$

How?

#### Proof by picture

Let $D^{\prime} = \\{B_1, B_2, B_3, B_4\\}$. So $B_1, B_2, B_3, B_4$ are disjoint or mutually exclusive.

<img src="{{ site.url }}/images/orf309/marginalization.png" style="width: 300px;"/>

(Here, I am using Ramon's image and needed to change the notation from $Y$ to $B$.)

# Properties of expectation 

### Linearity of expectation

An important property!

Let $X$, $Y$ be discrete random variables, then the following is true:

$$
\mathbb{E}(X + Y) = \mathbb{E}(X) + \mathbb{E}(Y)
$$

- If you think of $\mathbb{E}$ as an average, this is obvious.
- But it is _not obvious_ given how we defined $\mathbb{E}$.

#### Example

$$
\phi(x,y) = x+y
$$

$$
\mathbb{E}(X + Y)
= \mathbb{E}(\phi(x, y))
= \sum_{x \in D, y \in D^{\prime}} (x+y) \mathbb{P}(\{X=x, Y=y\})
$$

$$
= \sum_{x \in D, y \in D^{\prime}} x \mathbb{P}(\{X=x, Y=y\}) + \sum_{x \in D, y \in D^{\prime}} y \mathbb{P}(\{X=x, Y=y\})
$$

$$
= \sum_{x \in D} x \mathbb{P}(\{X=x\}) + \sum_{y \in D^{\prime}} y \mathbb{P}(\{Y=y\})
$$

$$
= \mathbb{E}(X) + \mathbb{E}(Y)
$$

### Indicator function (important!)

> **Definition.** Let $A$ be an event. The indicator function is the $\\{0,1\\}$-valued random variable such that:
>
> $$
> \mathbb{1}_A = \begin{cases} 1 & \text{If $A$ happens} \\
> 0 & \text{If $A$ does not happen} 
> \end{cases}
> $$
>
> Formally:
>
> $$
> \mathbb{1}_A = \begin{cases} 1 & \omega \in A \\
> 0 & \omega \notin A
> \end{cases}
> $$

What is special about this random variable?

$$
\mathbb{E}(\mathbb{1}_A) = 1 \cdot \mathbb{P}(\{\mathbb{1}_A=1\}) + 0 \cdot \mathbb{P}(\{\mathbb{1}_A=0\}) = \mathbb{P}(A)
$$

Ramon: "Probabilities are special cases of expectations."

### Example

Suppose you flip $n$ coins, and each comes up heads with probability $p$.

**Question:** what is the expected number of heads?

$$
E_i = \{\text{Event that $i$th coin is heads}\}
$$

$$
\mathbb{P}(E_i) = p \text{, for all $i$}
$$

$$
X = \text{Number of heads in $n$ flips}
$$

Notice

- We did not say that coins are independent
- We are given the marginal but not the joint distribution. Be careful!
- If you are not convinced that this matters, imagine that we have a completely dependent process in which you completely mimic the result of the first coin flip for all proceeding coin flips. Would your joint be different than if the coin flips were independent? Yes!

#### Claim

$$
X = \mathbb{1}_{E_1} + \mathbb{1}_{E_2} + ... + \mathbb{1}_{E_n} \leftarrow \text{Useful trick}
$$

$$
\begin{align}
\mathbb{E}(X) &= \mathbb{E}(\mathbb{1}_{E_1}) + \mathbb{E}(\mathbb{1}_{E_2}) + ... + \mathbb{E}(\mathbb{1}_{E_n}) \rightarrow \text{By the linearity of expectation}\\
&=np
\end{align}
$$

#### Remark

If $X \in \\{0,1,...,n\\}$:

$$
\mathbb{E}(X) = \sum_{k=0}^{n} k \cdot \mathbb{P}(\{X=k\})
$$

What we don't know given our problem statement is $\mathbb{P}(\{X=k\})$.

The nice thing about our trick is that we did not need to know it in order to compute $\mathbb{E}(X) = np$.

#### Example of what was just shown

Suppose the 2nd, 3rd, and 5th coins are heads and the rest are tails, $n=6$.

$$
\begin{align}
X & = \mathbb{1}_{E_1} + \mathbb{1}_{E_2} + \mathbb{1}_{E_3} + \mathbb{1}_{E_4} + \mathbb{1}_{E_5} + \mathbb{1}_{E_6}\\
& = 0 + 1 + 1 + 0 + 1 + 0\\
& = 3
\end{align}
$$

---
title: Lecture 6 - Expectation and distributions (UNDONE)
layout: orf309
date: 2017-02-24
---

# Review

$X: \Omega \rightarrow D$ is a discrete random variable.

$\phi: D \rightarrow \mathbb{R}$ (This maps $D$ to a numerical value).

**Expectation:**

$$
\mathbb{E}[\phi(x)] = \sum_{x \in D} \phi(x) \mathbb{P})(\{X=x\})
$$

> **Definition.** The **distribution** of a discrete random variable $X: \Omega \rightarrow D$ is given by $(\mathbb{P}(\\{X=x\\}))_{x \in D}$

The distribution is the information you need to compute all possible statistics $\mathbb{E}[\phi(x)]$ for all $\phi$.

> **Definition.** If $D$ is a finite set, a random variable such that:
>
> $$
> \mathbb{P}(\{X=x\}) = \frac{1}{|D|}
> $$
>
> For all $x \in D$ is said to be **uniformly distributed** on $D$.
>
> This is equivalent to saying it has the **uniform distribution**.

### Joint distribution

Suppose $X$, $Y$ are both uniformly distributed on $\\{0,1\\}$.

- $\mathbb{P}(\{X=0\}) = \frac{1}{2}$
- $\mathbb{P}(\{Y=0\}) = \frac{1}{2}$
- $\mathbb{P}(\{X=1\}) = \frac{1}{2}$
- $\mathbb{P}(\{Y=1\}) = \frac{1}{2}$

Question: $\mathbb{P}(\\{X=0, Y=1\\}) =$ ?

Answer: we _cannot know this_ from the probabilities. It could be anything. For example, imagine that $X$ and $Y$ are independent vs. dependent. Depending on that assumption, we would have a different joint probability.

What information do we need to compute:

$$
\mathbb{E}[\phi(X,Y)]
$$

For discrete random variables $X$, $Y$ where $X \in D$ and $Y \in D^{\prime}$.

We can define $Z = (X,Y)$ as a new random variable such that:

$$
Z \in \{(x,y), x \in D, y \in D^{\prime}\}
$$

So:

$$
\mathbb{E}[\phi(X, Y)]
= \mathbb{E}[\phi(Z)]
= \sum_{x \in D, y \in D^{\prime}} \phi(x, y) \mathbb{P}(\{X=x, Y=y\})
$$

This is the **joint distribution**.

> **Definition.** The **joint distribution** of a discrete random variable $X: \Omega \rightarrow D$ and $Y: \Omega \rightarrow D^{\prime}$ is given by:
>
> $$
> (\mathbb{P}(\{X=x, Y=y\}))_{x \in D, y \in D^{\prime}}
> $$
>
> We can do this with any number of random variables.

The above contains the information you need to compute $\mathbb{E}[\phi(X,Y)]$ for all $\phi$.

#### Remark

1. Knowing the distribution of $X$ and the distribution of $Y$ is _not_ enough information to specify the joint distribution of $X$ and $Y$.

2. Knowing the joint distribution of $X$, $Y$ should be enough informatoin to compute the distribution of $X$ and the distribution of $Y$. Just define a function $\phi(X,Y)$ that only depends on $X$.



### Marginal distribution

Point #2 above is called the **marginal distribution**:

$$
\mathbb{P}(\{X=x\}) = \sum_{y \in D^{\prime}} \mathbb{P}(\{X=x, Y=y\})
$$

How?

#### Proof by picture

Let $D^{\prime} = \\{B_1, B_2, B_3, B_4\\}$. So $B_1, B_2, B_3, B_4$ are disjoint or mutually exclusive.

<img src="{{ site.url }}/images/orf309/marginalization.png" style="width: 300px;"/>

(Here, I am using Ramon's image and needed to change the notation from $Y$ to $B$.)

### Linearity of expectation

An important property!

Let $X$, $Y$ be discrete random variables, then the following is true:

$$
\mathbb{E}[X + Y] = \mathbb{E}[X] + \mathbb{E}[Y]
$$

- If you think of $\mathbb{E}$ as an average, this is obvious.
- But it is _not obvious_ given how we defined $\mathbb{E}$.

#### Example

$$
\phi(x,y) = x+y
$$

$$
\mathbb{E}[X + Y] 
= \mathbb{E}[\phi(x, y)]
= \sum_{x \in D, y \in D^{\prime}} (x+y) \mathbb{P}(\{X=x, Y=y\})
$$

$$
= \sum_{x \in D, y \in D^{\prime}} x \mathbb{P}(\{X=x, Y=y\}) + \sum_{x \in D, y \in D^{\prime}} y \mathbb{P}(\{X=x, Y=y\})
$$

$$
= \sum_{x \in D} x \mathbb{P}(\{X=x\}) + \sum_{y \in D^{\prime}} y \mathbb{P}(\{Y=y\})
$$

$$
= \mathbb{E}(X) + \mathbb{E}(Y)
$$